# -*- coding: utf-8 -*-
"""medgemma-gemma3-test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14cP7gnmDlefjQ8bdE528uDwAjM56Ct_H
"""

#! pip install --upgrade --quiet accelerate bitsandbytes transformers Pillow

# Test of medgemma 4b on polyp images. We also compare it to the non-medical gemma3 4b.
# Model and more info here: https://huggingface.co/google/medgemma-4b-it

import os
import sys

google_colab = "google.colab" in sys.modules and not os.environ.get("VERTEX_PRODUCT")
if google_colab:
    # Use secret if running in Google Colab
    from google.colab import userdata
    hf_token = userdata.get("HF_TOKEN")
    if not hf_token:
        raise ValueError("HF_TOKEN secret not found. Please set it in Colab Secrets.")
    os.environ["HF_TOKEN"] = hf_token
else:
    # Store Hugging Face data under `/content` if running in Colab Enterprise
    if os.environ.get("VERTEX_PRODUCT") == "COLAB_ENTERPRISE":
        os.environ["HF_HOME"] = "/content/hf"
    # Authenticate with Hugging Face
    from huggingface_hub import get_token, notebook_login
    if get_token() is None:
        notebook_login()

from transformers import BitsAndBytesConfig
import torch

#27b model is text only!
medgemma_model_variant = "4b-it"  # @param ["4b-it", "27b-text-it"]
medgemma_model_id = f"google/medgemma-{medgemma_model_variant}"

use_quantization = True  # @param {type: "boolean"}

if "27b" in medgemma_model_variant and google_colab:
    if not ("A100" in torch.cuda.get_device_name(0) and use_quantization):
        raise ValueError(
            "Runtime has insufficient memory to run the 27B MedGemma variant. "
            "Please select an A100 GPU and use 4-bit quantization."
        )

medgemma_model_kwargs = dict(
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
if use_quantization:
    medgemma_model_kwargs["quantization_config"] = BitsAndBytesConfig(load_in_4bit=True)

gemma3_model_id = "google/gemma-3-4b-it" # Standard Gemma 3 4B instruction-tuned
gemma3_model_kwargs = dict(
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

if "text" in medgemma_model_variant:
    print(
        "MedGemma model selected is text-only. Image processing will be skipped for MedGemma."
    )
# Note: Gemma 3 4b-it is multimodal, so it will always attempt image processing.

from google.colab import files
from PIL import Image
from IPython.display import Image as IPImage, display, Markdown
import io

print("Please upload your polyp image.")
uploaded = files.upload()

if not uploaded:
    raise ValueError("No image uploaded. Please upload an image to continue.")

uploaded_image_filename = list(uploaded.keys())[0]
print(f"Uploaded image: {uploaded_image_filename}")

uploaded_image_bytes = uploaded[uploaded_image_filename]
# Store the PIL image object to be reused
pil_image = Image.open(io.BytesIO(uploaded_image_bytes))

# This prompt will be used for both models
shared_prompt = "Describe the characteristics of this polyp. For example, its size, shape, color, surface, and any notable features."

display(Markdown(f"Using shared prompt: **{shared_prompt}**"))
display(pil_image.resize((300, int(pil_image.height * 300 / pil_image.width))))

# Non medgemma model (https://huggingface.co/google/medgemma-4b-it)
if "text" not in medgemma_model_variant:
    # Format conversation for MedGemma
    medgemma_system_instruction = "You are an expert endoscopist and pathologist analyzing a polyp image."
    medgemma_messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": medgemma_system_instruction}]
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": shared_prompt},
                {"type": "image", "image": pil_image.copy()}, # Pass a copy of the PIL image
            ]
        }
    ]

    # Run MedGemma model directly
    from transformers import AutoModelForImageTextToText, AutoProcessor

    print(f"\nLoading MedGemma model: {medgemma_model_id}...")
    medgemma_model = AutoModelForImageTextToText.from_pretrained(
        medgemma_model_id,
        **medgemma_model_kwargs,
    )
    medgemma_processor = AutoProcessor.from_pretrained(medgemma_model_id)
    print("MedGemma model loaded.")

    print("Processing input for MedGemma...")
    medgemma_inputs = medgemma_processor.apply_chat_template(
        medgemma_messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(medgemma_model.device, dtype=torch.bfloat16) # Ensure dtype consistency

    print("Running MedGemma inference...")
    medgemma_input_len = medgemma_inputs["input_ids"].shape[-1]
    with torch.inference_mode():
        medgemma_generation = medgemma_model.generate(**medgemma_inputs, max_new_tokens=300, do_sample=False)
        medgemma_generation = medgemma_generation[0][medgemma_input_len:]

    medgemma_decoded = medgemma_processor.decode(medgemma_generation, skip_special_tokens=True)

    display(Markdown(f"\n\n---\n\n**[ User (to MedGemma) ]**\n\n{shared_prompt}"))
    buffered_medgemma = io.BytesIO()
    pil_image.save(buffered_medgemma, format=pil_image.format or "PNG")
    display(IPImage(data=buffered_medgemma.getvalue(), height=300))
    display(Markdown(f"\n\n---\n\n**[ MedGemma ({medgemma_model_id}) ]**\n\n{medgemma_decoded}\n\n---\n\n"))

    # Clean up to free memory before loading the next model, sometimes this does not work...
    del medgemma_model, medgemma_processor, medgemma_inputs, medgemma_generation
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("MedGemma resources released.")
else:
    display(Markdown("**MedGemma model is text-only, skipping image inference for MedGemma.**"))

# Non medical gemma3 model (https://huggingface.co/google/gemma-3-4b-it)
# Ensure previous model is fully cleared
if 'medgemma_model' in locals() or 'medgemma_model' in globals(): del medgemma_model
if 'medgemma_processor' in locals() or 'medgemma_processor' in globals(): del medgemma_processor
if 'medgemma_inputs' in locals() or 'medgemma_inputs' in globals(): del medgemma_inputs
if 'medgemma_generation' in locals() or 'medgemma_generation' in globals(): del medgemma_generation

import gc
gc.collect()

if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("Ensured previous MedGemma resources are cleared and CUDA cache emptied.")
# End of extra cleanup

# Standard Gemma 3 inference using pipeline API
from transformers import pipeline, BitsAndBytesConfig
import torch

print(f"\nInitializing Gemma 3 pipeline for model: {gemma3_model_id}")

gemma3_pipeline_model_kwargs = {
    "torch_dtype": torch.bfloat16,
}

if use_quantization:
    gemma3_pipeline_model_kwargs["quantization_config"] = BitsAndBytesConfig(load_in_4bit=True)
    print("Applying 4-bit quantization to Gemma 3 via pipeline's model_kwargs.")

try:
    gemma3_pipe = pipeline(
        "image-text-to-text",
        model=gemma3_model_id,
        model_kwargs=gemma3_pipeline_model_kwargs,
    )
    print("Gemma 3 pipeline initialized successfully.")

    # Format conversation for Gemma 3
    gemma3_system_instruction = "You are a helpful multimodal assistant. Analyze the provided image and text."
    # Note: the pipeline expects the image to be passable directly in the messages
    gemma3_messages_for_pipeline = [
        {
            "role": "system",
            "content": [{"type": "text", "text": gemma3_system_instruction}]
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": shared_prompt},    # Using the same prompt
                {"type": "image", "image": pil_image.copy()}, # Pass the PIL image object
            ]
        }
    ]

    print("Running Gemma 3 inference via pipeline...")
    # The 'text' argument to the pipe call actually takes the 'messages' list for chat models
    gemma3_output = gemma3_pipe(gemma3_messages_for_pipeline, max_new_tokens=300) # Removed do_sample as pipeline handles defaults

    # Extract the response
    # The output structure for image-text-to-text with chat can be a bit nested
    if gemma3_output and isinstance(gemma3_output, list) and "generated_text" in gemma3_output[0]:
        if isinstance(gemma3_output[0]["generated_text"], list) and len(gemma3_output[0]["generated_text"]) > 0:
             # If generated_text is a list (like in chat conversations)
            gemma3_decoded_response = gemma3_output[0]["generated_text"][-1]["content"]
        elif isinstance(gemma3_output[0]["generated_text"], str):
            # If generated_text is a simple string (less common for chat formats but possible)
            gemma3_decoded_response = gemma3_output[0]["generated_text"]
        else:
            gemma3_decoded_response = "Error: Could not parse Gemma 3 pipeline output structure."
            print("Unexpected Gemma 3 output structure:", gemma3_output)
    else:
        gemma3_decoded_response = "Error: Gemma 3 pipeline did not return expected output."
        print("Gemma 3 pipeline output:", gemma3_output)


    # Display output
    display(Markdown(f"\n\n---\n\n**[ User (to Gemma 3 via Pipeline) ]**\n\n{shared_prompt}"))
    buffered_gemma3_pipe = io.BytesIO()
    pil_image.save(buffered_gemma3_pipe, format=pil_image.format or "PNG")
    display(IPImage(data=buffered_gemma3_pipe.getvalue(), height=300))
    display(Markdown(f"\n\n---\n\n**[ Gemma 3 ({gemma3_model_id} via Pipeline) ]**\n\n{gemma3_decoded_response}\n\n---\n\n"))

except Exception as e:
    print(f"An error occurred during Gemma 3 pipeline inference: {e}")
    import traceback
    traceback.print_exc()
    display(Markdown(f"\n\n---\n\n**Gemma 3 ({gemma3_model_id} via Pipeline) FAILED**\n\nError: {e}\n\n---\n\n"))


finally:
    # Clean up Gemma 3 pipeline resources
    if 'gemma3_pipe' in locals():
        del gemma3_pipe
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("Gemma 3 pipeline resources attempted to be released.")